meta:
  email: ''
study:
  name: ClearviewAI Misconfiguration
  summary: >
    Clearview AI's source code repository, though password protected, was
    misconfigured to allow an arbitrary user to register an account.

    This allowed an external researcher to gain access to a private code
    repository that contained Clearview AI production credentials, keys to cloud
    storage buckets containing 70K video samples, and copies of its applications
    and Slack tokens.

    With access to training data, a bad-actor has the ability to cause an
    arbitrary misclassification in the deployed model.

    These kinds of attacks illustrate that any attempt to secure ML system
    should be on top of "traditional" good cybersecurity hygiene such as locking
    down the system with least privileges, multi-factor authentication and
    monitoring and auditing.
  incident-date: 2020-04-16T00:00:00.000Z
  incident-date-granularity: DATE
  procedure:
    - tactic: TA0001
      technique: T1078
      description: >+
        In this scenario, a security researcher gained initial access to via a
        valid account that was created through a misconfiguration.

  reported-by: Mossab Hussein (@mossab_hussein)
  references:
    - sourceDescription: ''
      url: https://techcrunch.com/2020/04/16/clearview-source-code-lapse/amp/
    - sourceDescription: ''
      url: >-
        https://gizmodo.com/we-found-clearview-ais-shady-face-recognition-app-1841961772
